import streamlit as st
import json
import random
import re # Pour le nettoyage potentiel du JSON
from . import gemini_client
from . import config
from . import utils

# Cl√©s pour st.session_state (inchang√©es)
QUIZ_QUESTIONS_KEY = "quiz_questions"
QUIZ_ANSWERS_KEY = "quiz_user_answers"
QUIZ_SCORE_KEY = "quiz_score"
QUIZ_CURRENT_QUESTION_KEY = "quiz_current_question_index"
QUIZ_FEEDBACK_KEY = "quiz_feedback"
QUIZ_GENERATED_KEY = "quiz_generated"
LAST_QUIZ_RAW_RESPONSE_KEY = "last_quiz_raw_response"
DOCUMENT_CONTEXT_KEY = "document_text" # Assumer que le texte du doc est ici

# --- Fonctions de G√©n√©ration (generate_quiz_questions reste identique √† la version pr√©c√©dente) ---
def generate_quiz_questions(text: str,
                            num_questions: int = 5,
                            quiz_type: str = "QCM",
                            difficulty: str = "Moyen",
                            model_name: str = config.DEFAULT_TEXT_MODEL_NAME) -> list | None:
    """
    G√©n√®re une liste de questions de quiz bas√©es sur le texte fourni.
    (Identique √† la version pr√©c√©dente avec prompt am√©lior√©)
    """
    if not text:
        st.warning("Le texte source pour le quiz est vide.", icon="‚ö†Ô∏è")
        return None

    if not gemini_client.configure_gemini():
         st.error("Impossible de g√©n√©rer le quiz car le client Gemini n'est pas configur√©.", icon="‚ùå")
         return None

    quiz_type_desc = config.QUIZ_TYPES.get(quiz_type, "Questions √† Choix Multiples")
    difficulty_desc = config.QUIZ_DIFFICULTY.get(difficulty, "moyen")

    # Limiter la taille du contexte pour la g√©n√©ration (√©vite erreurs/co√ªts excessifs)
    max_gen_context = 10000
    context_truncated = len(text) > max_gen_context
    text_for_prompt = text[:max_gen_context]

    prompt_parts = [
        f"Objectif : G√©n√©rer un quiz de {num_questions} questions de type '{quiz_type_desc}' (difficult√©: {difficulty_desc}) bas√© **strictement** sur le document suivant.",
        "--- DEBUT DOCUMENT ---",
        text_for_prompt,
        "--- FIN DOCUMENT ---" + (" (TRONQU√â)" if context_truncated else ""),
        "\n--- FORMAT DE SORTIE EXIGE ---",
        "1. R√©ponds **UNIQUEMENT** avec une liste JSON valide commen√ßant par `[` et se terminant par `]`.",
        "2. N'ajoute **AUCUN** texte, commentaire, explication ou formatage (comme ```json) avant ou apr√®s la liste JSON.",
        "3. Assure-toi que chaque objet JSON dans la liste est s√©par√© par une virgule `,` (sauf le dernier).",
        "4. Assure-toi que toutes les cha√Ænes de caract√®res dans le JSON sont correctement √©chapp√©es (guillemets doubles `\"`).",
        "5. La structure de chaque objet JSON d√©pend du type de quiz demand√© :"
    ]
    if quiz_type == "QCM":
        prompt_parts.extend([
            "   Pour QCM : `{ \"question\": \"...\", \"options\": [\"...\", \"...\", ...], \"correct_answer\": \"...\", \"explanation\": \"...\" }`",
            "   - 'options' doit √™tre une liste d'au moins 3 cha√Ænes.",
            "   - 'correct_answer' doit correspondre EXACTEMENT √† l'une des cha√Ænes dans 'options'."
        ])
    elif quiz_type == "Vrai/Faux":
        prompt_parts.extend([
            "   Pour Vrai/Faux : `{ \"question\": \"...\", \"correct_answer\": true/false, \"explanation\": \"...\" }`",
            "   - 'correct_answer' doit √™tre un bool√©en JSON (`true` ou `false`, sans guillemets)."
        ])
    elif quiz_type == "Ouvertes":
        prompt_parts.extend([
            "   Pour Ouvertes : `{ \"question\": \"...\", \"ideal_answer_points\": [\"...\", \"...\", ...], \"explanation\": \"...\" }`",
            "   - 'ideal_answer_points' est une liste de cha√Ænes d√©crivant les points cl√©s attendus."
        ])
    prompt_parts.append("\n--- EXEMPLE (pour QCM) ---")
    prompt_parts.append("""
[
  {"question": "Exemple Q1?", "options": ["A", "B", "C"], "correct_answer": "B", "explanation": "Expl. Q1"},
  {"question": "Exemple Q2?", "options": ["X", "Y", "Z"], "correct_answer": "X", "explanation": "Expl. Q2"}
]
""")
    prompt_parts.append("\n--- IMPORTANT ---")
    prompt_parts.append(f"G√©n√®re exactement {num_questions} questions. Commence ta r√©ponse directement par `[`.")
    full_prompt = "\n".join(prompt_parts)

    response = gemini_client.generate_text(prompt=full_prompt, model_name=model_name, temperature=0.4, max_output_tokens=3072)
    st.session_state[LAST_QUIZ_RAW_RESPONSE_KEY] = response

    if not response:
        st.error("La g√©n√©ration du quiz a √©chou√© (pas de r√©ponse de l'IA).", icon="‚ùå")
        return None

    try:
        processed_response = response.strip()
        if processed_response.startswith("```json"): processed_response = processed_response[7:]
        if processed_response.startswith("```"): processed_response = processed_response[3:]
        if processed_response.endswith("```"): processed_response = processed_response[:-3]
        processed_response = processed_response.strip()
        json_start = processed_response.find('[')
        json_end = processed_response.rfind(']') + 1
        if json_start == -1 or json_end == -1:
            st.error("Erreur Format JSON : Impossible d'extraire une liste JSON (`[...]`) de la r√©ponse de l'IA.", icon="‚ùå")
            return None

        json_str = processed_response[json_start:json_end]
        quiz_data = json.loads(json_str)

        if not isinstance(quiz_data, list) or len(quiz_data) == 0:
            st.error("Erreur Format JSON : L'IA n'a pas retourn√© une liste de questions valide.", icon="‚ùå")
            return None

        # Validation basique (peut √™tre renforc√©e)
        first_q = quiz_data[0]
        q_type_detected = detect_quiz_type(first_q) # Utiliser la d√©tection pour v√©rifier
        required_keys = []
        if q_type_detected == "QCM": required_keys = ['question', 'options', 'correct_answer', 'explanation']
        elif q_type_detected == "Vrai/Faux": required_keys = ['question', 'correct_answer', 'explanation']
        elif q_type_detected == "Ouvertes": required_keys = ['question', 'ideal_answer_points']

        if not required_keys or not all(key in first_q for key in required_keys):
             st.error(f"Erreur Format JSON : Les cl√©s attendues pour le type '{q_type_detected}' ne sont pas toutes pr√©sentes.", icon="‚ùå")
             return None

        st.success(f"Quiz de {len(quiz_data)} questions ({quiz_type}) g√©n√©r√© avec succ√®s !", icon="üß†")
        random.shuffle(quiz_data)
        return quiz_data

    except json.JSONDecodeError as e:
        st.error(f"Erreur de d√©codage JSON : L'IA a retourn√© un format invalide. D√©tails : {e}", icon="‚ùå")
        return None
    except Exception as e:
        st.error(f"Erreur inattendue lors du traitement de la r√©ponse du quiz : {e}", icon="üî•")
        return None


# --- Nouvelle Fonction pour √©valuer les r√©ponses ouvertes ---
# @st.cache_data # Attention au cache ici, l'√©valuation peut d√©pendre de l'√©tat actuel
def evaluate_open_ended_answer(user_answer: str, question_data: dict, document_context: str | None) -> str:
    """
    √âvalue une r√©ponse ouverte en utilisant Gemini et retourne le feedback.

    Args:
        user_answer (str): La r√©ponse fournie par l'utilisateur.
        question_data (dict): Les donn√©es de la question (incluant 'question' et 'ideal_answer_points').
        document_context (str | None): Le contexte du document original.

    Returns:
        str: Le feedback g√©n√©r√© par l'IA.
    """
    if not user_answer or not question_data:
        return "Impossible d'√©valuer : donn√©es manquantes."

    if not gemini_client.configure_gemini():
        return "√âvaluation impossible : client Gemini non configur√©."

    question_text = question_data.get('question', '')
    ideal_points = question_data.get('ideal_answer_points', [])
    explanation = question_data.get('explanation', '') # Contexte additionnel de la question

    # Pr√©parer le contexte (limit√© pour l'√©valuation)
    eval_context = ""
    if document_context:
        # Strat√©gie simple : prendre un extrait autour de mots cl√©s de la question ?
        # Ou juste les N premiers caract√®res comme pour le chat ? Prenons une limite raisonnable.
        max_eval_context = 4000
        eval_context = document_context[:max_eval_context]
        if len(document_context) > max_eval_context:
             eval_context += "\n[... CONTEXTE TRONQU√â ...]"

    # Construire le prompt d'√©valuation
    prompt_parts = [
        "R√¥le : Tu es un assistant p√©dagogique charg√© d'√©valuer la r√©ponse d'un utilisateur √† une question ouverte, en te basant **strictement** sur les points cl√©s attendus et le contexte du document fourni.",
        "\n--- CONTEXTE DU DOCUMENT (Source de v√©rit√©) ---",
        eval_context if eval_context else "[Aucun contexte fourni]",
        "--- FIN DU CONTEXTE ---",
        "\n--- QUESTION POS√âE ---",
        question_text,
        "\n--- POINTS CL√âS ATTENDUS DANS LA R√âPONSE ID√âALE ---",
        "- " + "\n- ".join(ideal_points) if ideal_points else "[Aucun point cl√© sp√©cifi√©]",
        f"\n--- EXPLICATION/CONTEXTE DE LA QUESTION (si disponible) ---",
        explanation if explanation else "[Aucune]",
        "\n--- R√âPONSE DE L'UTILISATEUR √Ä √âVALUER ---",
        user_answer,
        "\n--- INSTRUCTIONS D'√âVALUATION ---",
        "1. Compare la r√©ponse de l'utilisateur aux points cl√©s attendus ET au contexte du document.",
        "2. √âvalue la pertinence, l'exactitude et la compl√©tude de la r√©ponse.",
        "3. Fournis un feedback constructif et d√©taill√© en fran√ßais.",
        "4. Commence ton feedback par une appr√©ciation g√©n√©rale (ex: 'Correct.', 'Partiellement correct.', 'Incorrect.', 'Bonne tentative, mais...').",
        "5. Explique pourquoi la r√©ponse est correcte/incorrecte/partielle, en citant si possible des √©l√©ments du contexte ou des points cl√©s.",
        "6. Si la r√©ponse est proche mais manque des √©l√©ments, sugg√®re des am√©liorations ou les points manquants.",
        "7. Sois concis mais pr√©cis.",
        "\n--- FEEDBACK D√âTAILL√â (Commence ici) ---"
    ]
    full_prompt = "\n".join(prompt_parts)

    # Appeler Gemini pour l'√©valuation
    feedback_response = gemini_client.generate_text(
        prompt=full_prompt,
        temperature=0.5, # √âquilibr√© pour l'√©valuation
        max_output_tokens=512 # Assez pour un feedback d√©taill√©
    )

    if feedback_response:
        return feedback_response.strip()
    else:
        return "[Erreur lors de la g√©n√©ration du feedback par l'IA]"


# --- Fonctions d'initialisation et d'options (inchang√©es) ---
def initialize_quiz_state():
    if QUIZ_QUESTIONS_KEY not in st.session_state: st.session_state[QUIZ_QUESTIONS_KEY] = []
    if QUIZ_ANSWERS_KEY not in st.session_state: st.session_state[QUIZ_ANSWERS_KEY] = {}
    if QUIZ_SCORE_KEY not in st.session_state: st.session_state[QUIZ_SCORE_KEY] = 0
    if QUIZ_CURRENT_QUESTION_KEY not in st.session_state: st.session_state[QUIZ_CURRENT_QUESTION_KEY] = 0
    if QUIZ_FEEDBACK_KEY not in st.session_state: st.session_state[QUIZ_FEEDBACK_KEY] = {}
    if QUIZ_GENERATED_KEY not in st.session_state: st.session_state[QUIZ_GENERATED_KEY] = False
    if LAST_QUIZ_RAW_RESPONSE_KEY not in st.session_state: st.session_state[LAST_QUIZ_RAW_RESPONSE_KEY] = None
    # Assurer que la cl√© pour le contexte existe aussi
    if DOCUMENT_CONTEXT_KEY not in st.session_state: st.session_state[DOCUMENT_CONTEXT_KEY] = None


def reset_quiz_state():
    st.session_state[QUIZ_QUESTIONS_KEY] = []
    st.session_state[QUIZ_ANSWERS_KEY] = {}
    st.session_state[QUIZ_SCORE_KEY] = 0
    st.session_state[QUIZ_CURRENT_QUESTION_KEY] = 0
    st.session_state[QUIZ_FEEDBACK_KEY] = {}
    st.session_state[QUIZ_GENERATED_KEY] = False
    st.session_state[LAST_QUIZ_RAW_RESPONSE_KEY] = None

def display_quiz_options(text_available: bool):
    st.subheader("4. Quiz G√©n√©ratif")
    if not text_available:
        st.info("Chargez un document pour activer la g√©n√©ration de quiz.", icon="üìÑ")
        last_raw_response = st.session_state.get(LAST_QUIZ_RAW_RESPONSE_KEY)
        if last_raw_response:
             with st.expander("Afficher la derni√®re r√©ponse brute de l'IA (pour d√©bogage)"):
                  st.text_area("R√©ponse Brute", last_raw_response, height=200, key="last_quiz_raw_debug_no_text")
        return None, None, None, False

    col1, col2, col3 = st.columns(3)
    with col1:
        num_questions = st.number_input("Nombre de questions :", min_value=1, max_value=50, value=5, step=1, key="quiz_num_questions", help="Attention: Demander un tr√®s grand nombre de questions augmente le risque d'erreurs de formatage par l'IA.")
    with col2:
        quiz_type = st.selectbox("Type de questions :", options=list(config.QUIZ_TYPES.keys()), key="quiz_type_select")
    with col3:
        quiz_difficulty = st.selectbox("Difficult√© :", options=list(config.QUIZ_DIFFICULTY.keys()), index=1, key="quiz_difficulty_select")

    generate_button = st.button("G√©n√©rer le Quiz", key="generate_quiz_button", type="primary", use_container_width=True)
    if generate_button: reset_quiz_state()

    last_raw_response = st.session_state.get(LAST_QUIZ_RAW_RESPONSE_KEY)
    if last_raw_response and not st.session_state.get(QUIZ_GENERATED_KEY, False):
         with st.expander("Afficher la derni√®re r√©ponse brute de l'IA (pour d√©bogage)"):
              st.text_area("R√©ponse Brute", last_raw_response, height=200, key="last_quiz_raw_debug")

    return num_questions, quiz_type, quiz_difficulty, generate_button

# --- Fonctions d'affichage et d'√©valuation (modifi√©es pour √©valuation ouverte) ---

def detect_quiz_type(question_data: dict) -> str:
    """D√©tecte le type de question bas√© sur les cl√©s pr√©sentes (version am√©lior√©e)."""
    if not isinstance(question_data, dict): return "Inconnu"
    has_options = "options" in question_data and isinstance(question_data["options"], list)
    has_correct_answer = "correct_answer" in question_data
    has_ideal_points = "ideal_answer_points" in question_data and isinstance(question_data["ideal_answer_points"], list)
    if has_options and has_correct_answer: return "QCM"
    if has_correct_answer and isinstance(question_data["correct_answer"], bool): return "Vrai/Faux"
    if has_correct_answer and isinstance(question_data["correct_answer"], str) and question_data["correct_answer"].lower() in ['true', 'false']: return "Vrai/Faux"
    if has_ideal_points: return "Ouvertes"
    return "Inconnu"

def display_quiz_interface():
    """Affiche l'interface du quiz (questions, r√©ponses, score)."""
    initialize_quiz_state()
    questions = st.session_state.get(QUIZ_QUESTIONS_KEY, [])
    quiz_successfully_generated = st.session_state.get(QUIZ_GENERATED_KEY, False)

    if not questions or not quiz_successfully_generated:
        if not st.session_state.get(LAST_QUIZ_RAW_RESPONSE_KEY):
             st.info("G√©n√©rez un quiz √† partir d'un document pour commencer.", icon="üí°")
        return

    current_index = st.session_state.get(QUIZ_CURRENT_QUESTION_KEY, 0)
    total_questions = len(questions)

    if current_index >= total_questions:
        display_quiz_results()
        return

    st.progress((current_index + 1) / total_questions, text=f"Question {current_index + 1}/{total_questions}")
    question_data = questions[current_index]
    question_text = question_data.get("question", "Erreur: Texte de question manquant")
    quiz_type = detect_quiz_type(question_data)

    if quiz_type == "Inconnu":
         st.error(f"Erreur interne: Type de question non reconnu pour la question {current_index + 1}.", icon="üÜò")

    st.markdown(f"**Question {current_index + 1}:**")
    st.markdown(f"> {question_text}")

    user_answer = None
    answer_key = f"q_{current_index}_answer"
    widget_disabled = (quiz_type == "Inconnu")

    try:
        if quiz_type == "QCM":
            options = question_data.get("options", [])
            if not options or not isinstance(options, list): options = ["Erreur format"]
            user_answer = st.radio("Choisissez votre r√©ponse :", options, key=answer_key, index=None, disabled=widget_disabled)
        elif quiz_type == "Vrai/Faux":
            user_answer = st.radio("Votre r√©ponse :", [True, False], format_func=lambda x: "Vrai" if x else "Faux", key=answer_key, index=None, disabled=widget_disabled)
        elif quiz_type == "Ouvertes":
            user_answer = st.text_area("Votre r√©ponse :", key=answer_key, placeholder="Entrez votre r√©ponse ici...", disabled=widget_disabled)
    except Exception as display_e:
         st.error(f"Erreur lors de l'affichage des options de r√©ponse : {display_e}", icon="üÜò")
         widget_disabled = True # D√©sactiver soumission si affichage √©choue

    submit_button = st.button("Valider et Suivant", key=f"submit_q_{current_index}", type="primary", disabled=widget_disabled)

    if submit_button:
        # V√©rifier si une r√©ponse a √©t√© donn√©e (non vide pour Ouvertes)
        response_given = user_answer is not None
        if quiz_type == "Ouvertes" and isinstance(user_answer, str) and not user_answer.strip():
             response_given = False

        if response_given:
            st.session_state[QUIZ_ANSWERS_KEY][current_index] = user_answer
            is_correct = None # Reste None pour Ouvertes
            feedback_text = "Feedback non disponible."

            try:
                if quiz_type == "QCM" or quiz_type == "Vrai/Faux":
                    correct_answer = question_data.get("correct_answer")
                    # √âvaluation QCM/VF (logique pr√©c√©dente)
                    if quiz_type == "QCM":
                        is_correct = (user_answer == correct_answer)
                    elif quiz_type == "Vrai/Faux":
                        correct_bool = None
                        if isinstance(correct_answer, bool): correct_bool = correct_answer
                        elif isinstance(correct_answer, str):
                            if correct_answer.lower() == 'true': correct_bool = True
                            elif correct_answer.lower() == 'false': correct_bool = False
                        if correct_bool is not None: is_correct = (user_answer == correct_bool)
                        else: explanation = f"[Erreur format r√©ponse attendue: {correct_answer}] " + question_data.get('explanation', '')

                    # G√©n√©rer feedback QCM/VF
                    explanation = question_data.get('explanation', 'Pas d\'explication.')
                    if is_correct is True:
                        st.session_state[QUIZ_SCORE_KEY] += 1
                        feedback_text = f"‚úÖ **Correct !** {explanation}"
                    elif is_correct is False:
                        correct_ans_display = correct_answer if not isinstance(correct_answer, bool) else ("Vrai" if correct_answer else "Faux")
                        feedback_text = f"‚ùå **Incorrect.** La bonne r√©ponse √©tait : `{correct_ans_display}`. {explanation}"
                    else: # Erreur format r√©ponse correcte
                         feedback_text = f"‚ö†Ô∏è Impossible d'√©valuer (format r√©ponse attendue invalide). {explanation}"

                elif quiz_type == "Ouvertes":
                    # --- Appel √† l'√©valuation IA ---
                    with st.spinner("√âvaluation de la r√©ponse par l'IA..."):
                        document_context = st.session_state.get(DOCUMENT_CONTEXT_KEY)
                        feedback_text = evaluate_open_ended_answer(user_answer, question_data, document_context)
                    # is_correct reste None, on se base sur le texte du feedback

                # Stocker et afficher le feedback
                st.session_state[QUIZ_FEEDBACK_KEY][current_index] = (is_correct, feedback_text)
                if is_correct is True: st.success(feedback_text)
                elif is_correct is False: st.error(feedback_text)
                else: st.info(feedback_text) # Pour Ouvertes ou erreurs VF

                # Passer √† la question suivante
                st.session_state[QUIZ_CURRENT_QUESTION_KEY] += 1
                st.rerun()

            except Exception as eval_e:
                 st.error(f"Erreur lors de l'√©valuation/feedback : {eval_e}", icon="üÜò")
                 st.session_state[QUIZ_FEEDBACK_KEY][current_index] = (None, f"Erreur √©valuation: {eval_e}")
                 st.session_state[QUIZ_CURRENT_QUESTION_KEY] += 1
                 st.rerun()
        else:
            st.warning("Veuillez s√©lectionner ou entrer une r√©ponse avant de valider.", icon="‚ö†Ô∏è")


def display_quiz_results():
    """Affiche les r√©sultats finaux du quiz."""
    st.subheader("üèÅ R√©sultats du Quiz üèÅ")
    questions = st.session_state.get(QUIZ_QUESTIONS_KEY, [])
    answers = st.session_state.get(QUIZ_ANSWERS_KEY, {})
    feedback = st.session_state.get(QUIZ_FEEDBACK_KEY, {})
    score = st.session_state.get(QUIZ_SCORE_KEY, 0)
    total_questions = len(questions)
    evaluated_questions = sum(1 for i, fb in feedback.items() if i < total_questions and fb[0] is not None) # QCM/VF √©valu√©es

    if total_questions == 0:
         st.warning("Aucune question trouv√©e pour afficher les r√©sultats.")
         return

    if evaluated_questions > 0:
        percentage = (score / evaluated_questions) * 100
        st.metric("Score Final (QCM/Vrai-Faux)", f"{score}/{evaluated_questions}", f"{percentage:.1f}%")
    else:
        st.info("Aucune question QCM ou Vrai/Faux n'a √©t√© √©valu√©e.")

    st.markdown("---")
    st.markdown("**R√©capitulatif d√©taill√© :**")

    for i, question_data in enumerate(questions):
        q_text_short = question_data.get('question', f'Question {i+1}')[:60] + "..."
        with st.expander(f"Question {i+1}: {q_text_short}", expanded=False):
            st.markdown(f"**Question :** {question_data.get('question', 'N/A')}")
            user_ans = answers.get(i, "*Non r√©pondue*")
            fb = feedback.get(i) # Tuple (is_correct, feedback_text)
            q_type = detect_quiz_type(question_data)

            # Affichage r√©ponse utilisateur
            user_ans_display = "*Non r√©pondue*"
            if isinstance(user_ans, bool): user_ans_display = "`Vrai`" if user_ans else "`Faux`"
            elif user_ans is not None: user_ans_display = f"`{str(user_ans)}`" if q_type != "Ouvertes" else str(user_ans)

            if q_type != "Ouvertes":
                 st.write(f"Votre r√©ponse : {user_ans_display}")
            else:
                 st.write("Votre r√©ponse :")
                 # Utiliser st.markdown ou st.text pour les r√©ponses ouvertes
                 st.text(user_ans_display if user_ans_display != "*Non r√©pondue*" else user_ans_display)


            # Affichage feedback (qui contient d√©j√† la correction si n√©cessaire)
            if fb:
                 feedback_text = fb[1]
                 is_correct = fb[0] # Peut √™tre True, False, ou None (Ouvertes/Erreur)
                 if is_correct is True: st.success(feedback_text)
                 elif is_correct is False: st.error(feedback_text)
                 else: st.info(feedback_text) # Ouvertes ou Erreur d'√©valuation VF
            else:
                 # Fallback si pas de feedback (ne devrait pas arriver)
                 st.warning("Feedback non disponible.")
                 # Afficher manuellement la correction pour QCM/VF dans ce cas fallback
                 if q_type == "QCM" or q_type == "Vrai/Faux":
                     correct_ans = question_data.get('correct_answer', 'N/A')
                     correct_ans_display = correct_ans
                     if isinstance(correct_ans, bool): correct_ans_display = "`Vrai`" if correct_ans else "`Faux`"
                     elif correct_ans != 'N/A': correct_ans_display = f"`{correct_ans}`"
                     st.write(f"R√©ponse correcte (fallback): {correct_ans_display}")
                     st.write(f"Explication (fallback): {question_data.get('explanation', 'N/A')}")
                 elif q_type == "Ouvertes":
                      st.write(f"Points cl√©s attendus (fallback): {', '.join(question_data.get('ideal_answer_points', ['N/A']))}")

    # Boutons finaux
    col1, col2 = st.columns(2)
    with col1:
        if st.button("Recommencer un nouveau quiz", key="restart_quiz_button"):
            reset_quiz_state()
            st.rerun()


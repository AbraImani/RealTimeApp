import google.generativeai as genai
import streamlit as st
from PIL import Image # Pour le traitement d'images si multimodal
import io
from . import config

_gemini_client_initialized = False
_generative_model = None
_vision_model = None

def configure_gemini():
    """
    Configure l'API Google Gemini avec la cl√© API.
    Doit √™tre appel√©e une fois au d√©but.
    """
    global _gemini_client_initialized
    if not _gemini_client_initialized:
        try:
            if not config.GEMINI_API_KEY or config.GEMINI_API_KEY == "NO_KEY_CONFIGURED":
                 # L'erreur est d√©j√† g√©r√©e dans config.py ou app.py, on ne bloque pas ici
                 # mais les fonctions suivantes √©choueront probablement.
                 st.warning("Cl√© API Gemini non disponible. Les fonctionnalit√©s IA sont d√©sactiv√©es.", icon="‚ö†Ô∏è")
                 return False # Indique que la configuration a √©chou√©

            genai.configure(api_key=config.GEMINI_API_KEY)
            _gemini_client_initialized = True
            st.success("Client Gemini configur√© avec succ√®s.", icon="‚úÖ")
            return True
        except Exception as e:
            st.error(f"Erreur lors de la configuration de l'API Gemini : {e}", icon="üî•")
            _gemini_client_initialized = False
            return False
    return True # D√©j√† initialis√©

def get_generative_model(model_name: str = config.DEFAULT_TEXT_MODEL_NAME):
    """
    R√©cup√®re une instance initialis√©e du mod√®le g√©n√©ratif Gemini sp√©cifi√©.
    Configure le client si ce n'est pas d√©j√† fait.

    Args:
        model_name (str): Le nom du mod√®le √† utiliser (par d√©faut depuis config).

    Returns:
        genai.GenerativeModel: Une instance du mod√®le, ou None si erreur/non configur√©.
    """
    global _generative_model
    if not _gemini_client_initialized:
        if not configure_gemini():
            return None # La configuration a √©chou√©

    # Initialiser le mod√®le sp√©cifique si pas d√©j√† fait ou si le nom change
    # Note: Pourrait √™tre simplifi√© si on utilise toujours le m√™me mod√®le texte
    try:
        # Pour l'instant, on r√©utilise la m√™me instance si le nom est le m√™me
        # Si on voulait changer de mod√®le dynamiquement, il faudrait g√©rer √ßa
        if _generative_model is None or _generative_model.model_name != f"models/{model_name}":
             _generative_model = genai.GenerativeModel(model_name)
        return _generative_model
    except Exception as e:
        st.error(f"Erreur lors de l'initialisation du mod√®le Gemini '{model_name}': {e}", icon="ü§ñ")
        return None

# --- Fonctions d'interaction avec l'API ---

@st.cache_data(show_spinner="G√©n√©ration de la r√©ponse par l'IA...") # Cache la r√©ponse pour les m√™mes inputs
def generate_text(prompt: str,
                  model_name: str = config.DEFAULT_TEXT_MODEL_NAME,
                  temperature: float = 0.7,
                  max_output_tokens: int = 1024) -> str | None:
    """
    G√©n√®re du texte en utilisant le mod√®le Gemini sp√©cifi√©.

    Args:
        prompt (str): Le prompt √† envoyer au mod√®le.
        model_name (str): Le nom du mod√®le √† utiliser.
        temperature (float): Contr√¥le l'al√©atoire de la sortie (0.0 - 1.0).
        max_output_tokens (int): Nombre maximum de tokens √† g√©n√©rer.

    Returns:
        str: Le texte g√©n√©r√© par le mod√®le, ou None en cas d'erreur.
    """
    model = get_generative_model(model_name)
    if not model:
        st.error("Le mod√®le g√©n√©ratif Gemini n'est pas disponible.", icon="‚ùå")
        return None

    try:
        # Configuration de la g√©n√©ration
        generation_config = genai.types.GenerationConfig(
            temperature=temperature,
            max_output_tokens=max_output_tokens
            # top_p=0.9, # Autres param√®tres possibles
            # top_k=40
        )

        # Appel √† l'API Gemini
        response = model.generate_content(
            prompt,
            generation_config=generation_config,
            # safety_settings=... # Configuration de s√©curit√© si n√©cessaire
            )

        # V√©rifier si la r√©ponse contient du texte
        if response and response.candidates and response.candidates[0].content.parts:
             # Acc√©der correctement au texte g√©n√©r√©
             # La structure peut varier l√©g√®rement selon la version de l'API et le type de r√©ponse
            generated_text = "".join(part.text for part in response.candidates[0].content.parts if hasattr(part, 'text'))
            if generated_text:
                return generated_text.strip()
            else:
                 # G√©rer le cas o√π la r√©ponse est bloqu√©e ou vide
                 block_reason = response.prompt_feedback.block_reason if response.prompt_feedback else "Inconnue"
                 safety_ratings = response.prompt_feedback.safety_ratings if response.prompt_feedback else "N/A"
                 st.warning(f"La r√©ponse de Gemini √©tait vide ou bloqu√©e. Raison: {block_reason}. Safety Ratings: {safety_ratings}", icon="‚ö†Ô∏è")
                 return None

        else:
            # G√©rer le cas d'une r√©ponse inattendue ou vide
            st.warning(f"R√©ponse inattendue ou vide re√ßue de Gemini: {response}", icon="ü§î")
            return None

    except Exception as e:
        st.error(f"Erreur lors de l'appel √† l'API Gemini : {e}", icon="üî•")
        # Afficher des d√©tails suppl√©mentaires si disponibles (ex: erreur API sp√©cifique)
        # if hasattr(e, 'response'): st.error(f"D√©tails de l'erreur API: {e.response.text}")
        return None

# --- Fonction multimodale ---

# @st.cache_data(show_spinner="Analyse de l'image par l'IA...")
# def analyze_image(prompt: str, image_bytes: bytes, model_name: str = config.DEFAULT_VISION_MODEL_NAME) -> str | None:
#     """
#     Analyse une image avec un prompt texte en utilisant un mod√®le de vision Gemini.

#     Args:
#         prompt (str): La question ou l'instruction concernant l'image.
#         image_bytes (bytes): Le contenu binaire de l'image.
#         model_name (str): Le nom du mod√®le de vision √† utiliser.

#     Returns:
#         str: La r√©ponse textuelle de l'analyse, ou None en cas d'erreur.
#     """
#     global _vision_model
#     if not _gemini_client_initialized:
#         if not configure_gemini():
#             return None

#     try:
#         # Initialiser le mod√®le vision si n√©cessaire
#         if _vision_model is None or _vision_model.model_name != f"models/{model_name}":
#              _vision_model = genai.GenerativeModel(model_name)

#         # Pr√©parer l'image pour l'API
#         img = Image.open(io.BytesIO(image_bytes))

#         # Appel √† l'API multimodale
#         response = _vision_model.generate_content([prompt, img])

#         if response and response.candidates and response.candidates[0].content.parts:
#             generated_text = "".join(part.text for part in response.candidates[0].content.parts if hasattr(part, 'text'))
#             return generated_text.strip() if generated_text else None
#         else:
#             st.warning(f"R√©ponse inattendue ou vide re√ßue du mod√®le de vision Gemini: {response}", icon="ü§î")
#             return None

#     except Exception as e:
#         st.error(f"Erreur lors de l'analyse de l'image avec Gemini : {e}", icon="üî•")
#         return None

# --- Fonctions de Chat (si l'API supporte un mode conversationnel direct) ---

def start_chat_session(model_name: str = config.DEFAULT_TEXT_MODEL_NAME, history: list = None):
    """
    D√©marre une session de chat avec le mod√®le Gemini.

    Args:
        model_name (str): Le nom du mod√®le √† utiliser.
        history (list): Un historique optionnel de messages pr√©c√©dents.
                        Format attendu par l'API Gemini (ex: [{'role': 'user', 'parts': ['message']}, {'role': 'model', 'parts': ['response']}])

    Returns:
        genai.ChatSession: Une instance de session de chat, ou None si erreur.
    """
    model = get_generative_model(model_name)
    if not model:
        st.error("Le mod√®le g√©n√©ratif Gemini n'est pas disponible pour le chat.", icon="‚ùå")
        return None

    try:
        # S'assurer que l'historique est dans le bon format si fourni
        formatted_history = []
        if history:
            for msg in history:
                # V√©rifier et adapter le format si n√©cessaire
                # Ceci est un exemple, la structure exacte peut varier
                if isinstance(msg, dict) and 'role' in msg and 'parts' in msg:
                     # Assurez-vous que 'parts' est une liste de strings ou d'objets Part valides
                     parts_content = msg['parts']
                     if isinstance(parts_content, str):
                         formatted_history.append({'role': msg['role'], 'parts': [parts_content]})
                     elif isinstance(parts_content, list):
                          # V√©rifier que les √©l√©ments de la liste sont des strings ou des objets Part
                          # Pour simplifier, on suppose ici qu'ils sont des strings
                          if all(isinstance(p, str) for p in parts_content):
                              formatted_history.append({'role': msg['role'], 'parts': parts_content})
                          else:
                              st.warning(f"Format de message invalide dans l'historique: {msg}", icon="‚ö†Ô∏è")
                              # Ignorer ce message ou tenter de le corriger
                     else:
                         st.warning(f"Format de 'parts' invalide dans l'historique: {msg}", icon="‚ö†Ô∏è")

                else:
                     st.warning(f"Format de message invalide dans l'historique: {msg}", icon="‚ö†Ô∏è")


        chat = model.start_chat(history=formatted_history if formatted_history else None)
        return chat
    except Exception as e:
        st.error(f"Erreur lors du d√©marrage de la session de chat Gemini : {e}", icon="üî•")
        return None
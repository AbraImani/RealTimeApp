import streamlit as st
from . import gemini_client
from . import config
from . import utils

# Cl√© pour stocker l'historique dans st.session_state
CHAT_HISTORY_KEY = "gemini_chat_history"
# Cl√© pour stocker l'instance de chat si l'API le supporte
CHAT_SESSION_KEY = "gemini_chat_session"

# Limite de caract√®res du document √† injecter dans le contexte du chat
# Augment√©e, mais attention aux limites r√©elles du mod√®le utilis√© !
# Les mod√®les comme Gemini 1.5 Pro ont des fen√™tres de contexte beaucoup plus grandes,
# mais gemini-1.5-flash ou gemini-1.0-pro sont plus limit√©s.
# Une valeur comme 15000 est un compromis. Pour des documents tr√®s volumineux,
# une approche RAG serait n√©cessaire.
MAX_CONTEXT_LENGTH = 15000

def initialize_chat():
    """Initialise l'historique du chat dans st.session_state si n√©cessaire."""
    if CHAT_HISTORY_KEY not in st.session_state:
        st.session_state[CHAT_HISTORY_KEY] = []

def display_chat_history():
    """Affiche l'historique de la conversation dans l'interface Streamlit."""
    if CHAT_HISTORY_KEY in st.session_state:
        for message in st.session_state[CHAT_HISTORY_KEY]:
            # Utiliser "assistant" pour le r√¥le du mod√®le pour correspondre √† st.chat_message
            role = message["role"] if message["role"] == "user" else "assistant"
            with st.chat_message(role):
                st.markdown(message["content"])

def add_message_to_history(role: str, content: str):
    """Ajoute un message √† l'historique du chat."""
    if CHAT_HISTORY_KEY in st.session_state:
        # S'assurer que le r√¥le est 'user' ou 'model' pour la compatibilit√© API future
        api_role = "user" if role == "user" else "model"
        st.session_state[CHAT_HISTORY_KEY].append({"role": api_role, "content": content})

def clear_chat_history():
    """Efface l'historique du chat."""
    st.session_state[CHAT_HISTORY_KEY] = []
    st.success("Historique du chat effac√©.", icon="üßπ")

def get_chat_response(user_input: str,
                      document_context: str | None,
                      model_name: str = config.DEFAULT_TEXT_MODEL_NAME) -> str | None:
    """
    Obtient une r√©ponse du chatbot Gemini en utilisant l'historique et le contexte.

    Args:
        user_input (str): La derni√®re question ou message de l'utilisateur.
        document_context (str | None): Le texte du document √† utiliser comme contexte.
        model_name (str): Le nom du mod√®le Gemini √† utiliser.

    Returns:
        str: La r√©ponse g√©n√©r√©e par le mod√®le, ou None en cas d'erreur.
    """
    if not user_input:
        return None

    if not gemini_client.configure_gemini():
         st.error("Impossible d'obtenir une r√©ponse car le client Gemini n'est pas configur√©.", icon="‚ùå")
         return None

    prompt_parts = []
    prompt_parts.append("Tu es un assistant IA expert dans l'analyse de documents. R√©ponds aux questions de l'utilisateur en te basant PR√âCIS√âMENT et UNIQUEMENT sur le contexte du document fourni ci-dessous et l'historique de la conversation. Si l'information demand√©e n'est pas explicitement pr√©sente dans le document, indique que tu ne peux pas r√©pondre avec les informations fournies. Ne sp√©cule pas et ne cherche pas d'informations externes.")

    # Ajouter le contexte du document (s'il existe)
    if document_context:
        context_to_inject = document_context
        if len(document_context) > MAX_CONTEXT_LENGTH:
             # Tronquer le contexte et informer l'utilisateur plus clairement
             truncated_context = document_context[:MAX_CONTEXT_LENGTH]
             st.warning(
                 f"Le document est tr√®s long ({len(document_context)} caract√®res). "
                 f"Seuls les {MAX_CONTEXT_LENGTH} premiers caract√®res seront utilis√©s comme contexte pour cette conversation afin d'√©viter les erreurs et de respecter les limites de l'IA. "
                 "Pour une analyse compl√®te de documents tr√®s volumineux, des techniques plus avanc√©es (non impl√©ment√©es ici) seraient n√©cessaires.",
                 icon="‚ö†Ô∏è"
             )
             context_to_inject = truncated_context
        else:
             # Optionnel : informer que tout le contexte est utilis√©
             # st.info(f"Utilisation du contexte complet du document ({len(document_context)} caract√®res).", icon="‚ÑπÔ∏è")
             pass

        prompt_parts.append("\n--- CONTEXTE DU DOCUMENT (Baser la r√©ponse sur ce texte) ---")
        prompt_parts.append(context_to_inject)
        prompt_parts.append("--- FIN DU CONTEXTE DU DOCUMENT ---")

    else:
         prompt_parts.append("\n[INFO] Aucun document n'est charg√© pour fournir un contexte.")


    # Ajouter l'historique de la conversation (format simple)
    prompt_parts.append("\n--- HISTORIQUE DE LA CONVERSATION ---")
    if CHAT_HISTORY_KEY in st.session_state and st.session_state[CHAT_HISTORY_KEY]:
        for msg in st.session_state[CHAT_HISTORY_KEY]:
            role_prefix = "Utilisateur" if msg["role"] == "user" else "Assistant"
            prompt_parts.append(f"{role_prefix}: {msg['content']}")
    else:
        prompt_parts.append("(D√©but de la conversation)")
    prompt_parts.append("--- FIN DE L'HISTORIQUE ---")

    # Ajouter la derni√®re question de l'utilisateur
    prompt_parts.append(f"\nUtilisateur: {user_input}")
    # Indiquer √† l'IA o√π commencer sa r√©ponse et rappeler l'instruction cl√©
    prompt_parts.append("\nAssistant (R√©pondre en se basant **uniquement** sur le contexte fourni):")

    full_prompt = "\n".join(prompt_parts)

    # Appeler l'API Gemini
    response = gemini_client.generate_text(
        prompt=full_prompt,
        model_name=model_name,
        temperature=0.2, # Temp√©rature mod√©r√©e pour rester factuel mais fluide
        max_output_tokens=2500 # Augmenter un peu si n√©cessaire
    )

    if response:
        return response.strip()
    else:
        return None

def display_chat_interface(document_context: str | None):
    """
    Affiche l'interface compl√®te du chat dans Streamlit.

    Args:
        document_context (str | None): Le texte du document charg√©.
    """
    st.subheader("3. Chat Contextuel")

    initialize_chat()

    chat_enabled = bool(document_context)
    if not chat_enabled:
        st.info("Chargez un document pour d√©marrer une conversation contextuelle.", icon="üìÑ")

    # Zone d'affichage de l'historique
    chat_container = st.container(height=400, border=True)
    with chat_container:
        display_chat_history()

    # Zone de saisie utilisateur
    user_input = st.chat_input(
        "Posez votre question sur le document...",
        key="chat_user_input",
        disabled=not chat_enabled # D√©sactiver si pas de contexte
    )

    if user_input and chat_enabled:
        # 1. Ajouter et afficher le message utilisateur
        # Utiliser "user" pour l'affichage st.chat_message
        add_message_to_history("user", user_input)
        with chat_container:
             with st.chat_message("user"):
                 st.markdown(user_input)

        # 2. Obtenir la r√©ponse de l'IA
        with st.spinner("L'assistant r√©fl√©chit..."):
            ai_response = get_chat_response(user_input, document_context)

        # 3. Ajouter et afficher la r√©ponse de l'IA
        if ai_response:
            # Utiliser "assistant" pour l'affichage st.chat_message
            add_message_to_history("assistant", ai_response)
            with chat_container:
                 with st.chat_message("assistant"):
                     st.markdown(ai_response)
        else:
            # Afficher un message d'erreur si la r√©ponse a √©chou√©
            error_message = "D√©sol√©, une erreur s'est produite lors de la g√©n√©ration de la r√©ponse. V√©rifiez la configuration de l'API ou r√©essayez."
            add_message_to_history("assistant", error_message) # Ajouter l'erreur √† l'historique aussi
            with chat_container:
                 with st.chat_message("assistant"):
                     st.error(error_message, icon="üòï")

        # Forcer un rerun peut parfois aider √† rafra√Æchir l'UI imm√©diatement
        # st.rerun()

    # Bouton pour effacer l'historique
    if st.button("Effacer l'historique du Chat", key="clear_chat_button"):
        clear_chat_history()
        st.rerun() # Forcer le rafra√Æchissement de l'interface
